{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGuhAg+OLJyKl4hDSWIfdH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neo4j-contrib/ms-graphrag-neo4j/blob/main/examples/drift_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X1-Cvi4wYdu",
        "outputId": "8a866b56-1316-4c78-9563-c0fcd4b72a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade git+https://github.com/neo4j-contrib/ms-graphrag-neo4j.git tiktoken llama-index-core json-repair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from typing import List\n",
        "\n",
        "import requests\n",
        "import tiktoken\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.workflow import (\n",
        "    Event,\n",
        "    Context,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        ")\n",
        "from neo4j import GraphDatabase\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from ms_graphrag_neo4j import MsGraphRAG"
      ],
      "metadata": {
        "id": "zQyBvp-CweHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Neo4j Sandbox - Blank Project https://sandbox.neo4j.com/\n",
        "\n",
        "os.environ[\"NEO4J_URI\"]=\"bolt://44.197.242.70:7687\"\n",
        "os.environ[\"NEO4J_USERNAME\"]=\"neo4j\"\n",
        "os.environ[\"NEO4J_PASSWORD\"]=\"oars-condensation-sectors\""
      ],
      "metadata": {
        "id": "G2VeRJnASa1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"]= getpass(\"Openai API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMBFryYSwv-T",
        "outputId": "7e6bfed1-7508-4191-ea4d-3b25eb368e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Openai API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = AsyncOpenAI()\n",
        "\n",
        "driver = GraphDatabase.driver(\n",
        "    os.environ[\"NEO4J_URI\"],\n",
        "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"]),\n",
        "    notifications_min_severity=\"OFF\",\n",
        ")\n",
        "\n",
        "ms_graph = MsGraphRAG(driver=driver, model=\"gpt-5-mini\", max_workers=10)"
      ],
      "metadata": {
        "id": "i4Cob15XxIgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingestion"
      ],
      "metadata": {
        "id": "5ZhXj40gSgCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text"
      ],
      "metadata": {
        "id": "993eimpuxn_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=20)"
      ],
      "metadata": {
        "id": "TrRlaKR-y0y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntitySummarization(Event):\n",
        "    pass\n",
        "\n",
        "class CommunitySummarization(Event):\n",
        "    pass\n",
        "\n",
        "class CommunityEmbeddings(Event):\n",
        "    pass\n",
        "\n",
        "class EntityEmbeddings(Event):\n",
        "    pass"
      ],
      "metadata": {
        "id": "03xB5ecmWI_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_COMMUNITY_RATING = 3\n",
        "TEXT_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "\n",
        "\n",
        "class MSGraphRAGIngestion(Workflow):\n",
        "    @step\n",
        "    async def entity_extraction(self, ev: StartEvent) -> EntitySummarization:\n",
        "        chunks = splitter.split_text(ev.text)\n",
        "        await ms_graph.extract_nodes_and_rels(chunks, ev.allowed_entities)\n",
        "        return EntitySummarization()\n",
        "\n",
        "    @step\n",
        "    async def entity_summarization(\n",
        "        self, ev: EntitySummarization\n",
        "    ) -> CommunitySummarization:\n",
        "        await ms_graph.summarize_nodes_and_rels()\n",
        "        return CommunitySummarization()\n",
        "\n",
        "    @step\n",
        "    async def community_summarization(\n",
        "        self, ev: CommunitySummarization\n",
        "    ) -> CommunityEmbeddings:\n",
        "        await ms_graph.summarize_communities()\n",
        "        return CommunityEmbeddings()\n",
        "\n",
        "    @step\n",
        "    async def community_embeddings(self, ev: CommunityEmbeddings) -> EntityEmbeddings:\n",
        "        communities = ms_graph.query(\n",
        "            \"\"\"\n",
        "MATCH (c:__Community__)\n",
        "WHERE c.summary IS NOT NULL AND c.rating > $min_community_rating\n",
        "RETURN coalesce(c.title, \"\") + \" \" + c.summary AS community_description, c.id AS community_id\n",
        "\"\"\",\n",
        "            params={\"min_community_rating\": MIN_COMMUNITY_RATING},\n",
        "        )\n",
        "        if communities:\n",
        "            response = await client.embeddings.create(\n",
        "                input=[c[\"community_description\"] for c in communities],\n",
        "                model=TEXT_EMBEDDING_MODEL,\n",
        "            )\n",
        "            embeds = []\n",
        "            for community, embedding in zip(communities, response.data):\n",
        "                embeds.append(\n",
        "                    {\n",
        "                        \"community_id\": community[\"community_id\"],\n",
        "                        \"embedding\": embedding.embedding,\n",
        "                    }\n",
        "                )\n",
        "            ms_graph.query(\n",
        "                \"\"\"UNWIND $data as row\n",
        "            MATCH (c:__Community__ {id: row.community_id})\n",
        "            CALL db.create.setNodeVectorProperty(c, 'embedding', row.embedding)\"\"\",\n",
        "                params={\"data\": embeds},\n",
        "            )\n",
        "            ms_graph.query(\n",
        "                \"CREATE VECTOR INDEX community IF NOT EXISTS FOR (c:__Community__) ON c.embedding\"\n",
        "            )\n",
        "        else:\n",
        "            print(\"No community was summarized\")\n",
        "        return EntityEmbeddings()\n",
        "\n",
        "    @step\n",
        "    async def entity_embeddings(self, ev: EntityEmbeddings) -> StopEvent:\n",
        "        entities = ms_graph.query(\"\"\"\n",
        "    MATCH (e:__Entity__)\n",
        "    WHERE e.summary IS NOT NULL\n",
        "    RETURN coalesce(e.name, \"\") + \" \" + e.summary AS entity_description, e.name AS entity_name\n",
        "    \"\"\")\n",
        "        if entities:\n",
        "            response = await client.embeddings.create(\n",
        "                input=[e[\"entity_description\"] for e in entities],\n",
        "                model=TEXT_EMBEDDING_MODEL,\n",
        "            )\n",
        "            embeds = []\n",
        "            for entity, embedding in zip(entities, response.data):\n",
        "                embeds.append(\n",
        "                    {\n",
        "                        \"entity_name\": entity[\"entity_name\"],\n",
        "                        \"embedding\": embedding.embedding,\n",
        "                    }\n",
        "                )\n",
        "            ms_graph.query(\n",
        "                \"\"\"UNWIND $data as row\n",
        "            MATCH (e:__Entity__ {name: row.entity_name})\n",
        "            CALL db.create.setNodeVectorProperty(e, 'embedding', row.embedding)\"\"\",\n",
        "                params={\"data\": embeds},\n",
        "            )\n",
        "            ms_graph.query(\n",
        "                \"CREATE VECTOR INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON e.embedding\"\n",
        "            )\n",
        "        else:\n",
        "            print(\"No entity was summarized\")\n",
        "        return StopEvent()"
      ],
      "metadata": {
        "id": "lhnykbuPV8Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = MSGraphRAGIngestion(timeout=3600, verbose=False)\n",
        "result = await w.run(text=book, allowed_entities = [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"EVENT\", \"ARTIFACT\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jupgtWQ2UKqV",
        "outputId": "d5169b4f-7390-479d-f1d0-ec95cd969a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting nodes & relationships: 100%|██████████| 86/86 [07:03<00:00,  4.93s/it]\n",
            "Summarizing nodes: 100%|██████████| 101/101 [01:21<00:00,  1.23it/s]\n",
            "Summarizing relationships: 100%|██████████| 120/120 [01:33<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leiden algorithm identified 3 community levels with 22 communities on the last level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Summarizing communities: 100%|██████████| 22/22 [01:46<00:00,  4.85s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "QUYdV3APSj_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2024 Microsoft Corporation.\n",
        "# Licensed under the MIT License\n",
        "\n",
        "\"\"\"DRIFT Search prompts.\"\"\"\n",
        "\n",
        "DRIFT_LOCAL_SYSTEM_PROMPT = \"\"\"\n",
        "---Role---\n",
        "\n",
        "You are a helpful assistant responding to questions about data in the tables provided.\n",
        "\n",
        "\n",
        "---Goal---\n",
        "\n",
        "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
        "\n",
        "If you don't know the answer, just say so. Do not make anything up.\n",
        "\n",
        "Points supported by data should list their data references as follows:\n",
        "\n",
        "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
        "\n",
        "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
        "\n",
        "For example:\n",
        "\n",
        "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16)].\"\n",
        "\n",
        "where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
        "\n",
        "Pay close attention specifically to the Sources tables as it contains the most relevant information for the user query. You will be rewarded for preserving the context of the sources in your response.\n",
        "\n",
        "---Target response length and format---\n",
        "\n",
        "{response_type}\n",
        "\n",
        "\n",
        "---Data tables---\n",
        "\n",
        "{context_data}\n",
        "\n",
        "\n",
        "---Goal---\n",
        "\n",
        "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
        "\n",
        "If you don't know the answer, just say so. Do not make anything up.\n",
        "\n",
        "Points supported by data should list their data references as follows:\n",
        "\n",
        "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
        "\n",
        "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
        "\n",
        "For example:\n",
        "\n",
        "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16)].\"\n",
        "\n",
        "where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
        "\n",
        "Pay close attention specifically to the Sources tables as it contains the most relevant information for the user query. You will be rewarded for preserving the context of the sources in your response.\n",
        "\n",
        "---Target response length and format---\n",
        "\n",
        "{response_type}\n",
        "\n",
        "Add sections and commentary to the response as appropriate for the length and format.\n",
        "\n",
        "Additionally provide a score between 0 and 100 representing how well the response addresses the overall research question: {global_query}. Based on your response, suggest up to five follow-up questions that could be asked to further explore the topic as it relates to the overall research question. Do not include scores or follow up questions in the 'response' field of the JSON, add them to the respective 'score' and 'follow_up_queries' keys of the JSON output. Format your response in JSON with the following keys and values:\n",
        "\n",
        "{{'response': str, Put your answer, formatted in markdown, here. Do not answer the global query in this section.\n",
        "'score': int,\n",
        "'follow_up_queries': List[str]}}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "DRIFT_REDUCE_PROMPT = \"\"\"\n",
        "---Role---\n",
        "\n",
        "You are a helpful assistant responding to questions about data in the reports provided.\n",
        "\n",
        "---Goal---\n",
        "\n",
        "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input reports appropriate for the response length and format, and incorporating any relevant general knowledge while being as specific, accurate and concise as possible.\n",
        "\n",
        "If you don't know the answer, just say so. Do not make anything up.\n",
        "\n",
        "Points supported by data should list their data references as follows:\n",
        "\n",
        "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
        "\n",
        "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
        "\n",
        "For example:\n",
        "\n",
        "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (1, 5, 15)].\"\n",
        "\n",
        "Do not include information where the supporting evidence for it is not provided.\n",
        "\n",
        "If you decide to use general knowledge, you should add a delimiter stating that the information is not supported by the data tables. For example:\n",
        "\n",
        "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing. [Data: General Knowledge (href)]\"\n",
        "\n",
        "---Data Reports---\n",
        "\n",
        "{context_data}\n",
        "\n",
        "---Target response length and format---\n",
        "\n",
        "{response_type}\n",
        "\n",
        "\n",
        "---Goal---\n",
        "\n",
        "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input reports appropriate for the response length and format, and incorporating any relevant general knowledge while being as specific, accurate and concise as possible.\n",
        "\n",
        "If you don't know the answer, just say so. Do not make anything up.\n",
        "\n",
        "Points supported by data should list their data references as follows:\n",
        "\n",
        "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
        "\n",
        "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
        "\n",
        "For example:\n",
        "\n",
        "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (1, 5, 15)].\"\n",
        "\n",
        "Do not include information where the supporting evidence for it is not provided.\n",
        "\n",
        "If you decide to use general knowledge, you should add a delimiter stating that the information is not supported by the data tables. For example:\n",
        "\n",
        "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing. [Data: General Knowledge (href)]\".\n",
        "\n",
        "Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown. Now answer the following query using the data above:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "DRIFT_PRIMER_PROMPT = \"\"\"You are a helpful agent designed to reason over a knowledge graph in response to a user query.\n",
        "This is a unique knowledge graph where edges are freeform text rather than verb operators. You will begin your reasoning looking at a summary of the content of the most relevant communites and will provide:\n",
        "\n",
        "1. score: How well the intermediate answer addresses the query. A score of 0 indicates a poor, unfocused answer, while a score of 100 indicates a highly focused, relevant answer that addresses the query in its entirety.\n",
        "\n",
        "2. intermediate_answer: This answer should match the level of detail and length found in the community summaries. The intermediate answer should be exactly 2000 characters long. This must be formatted in markdown and must begin with a header that explains how the following text is related to the query.\n",
        "\n",
        "3. follow_up_queries: A list of follow-up queries that could be asked to further explore the topic. These should be formatted as a list of strings. Generate at least five good follow-up queries.\n",
        "\n",
        "Use this information to help you decide whether or not you need more information about the entities mentioned in the report. You may also use your general knowledge to think of entities which may help enrich your answer.\n",
        "\n",
        "You will also provide a full answer from the content you have available. Use the data provided to generate follow-up queries to help refine your search. Do not ask compound questions, for example: \"What is the market cap of Apple and Microsoft?\". Use your knowledge of the entity distribution to focus on entity types that will be useful for searching a broad area of the knowledge graph.\n",
        "\n",
        "For the query:\n",
        "\n",
        "{query}\n",
        "\n",
        "The top-ranked community summaries:\n",
        "\n",
        "{community_reports}\n",
        "\n",
        "Provide the intermediate answer, and all scores in JSON format following:\n",
        "\n",
        "{{'intermediate_answer': str,\n",
        "'score': int,\n",
        "'follow_up_queries': List[str]}}\n",
        "\n",
        "Begin:\n",
        "\"\"\"\n",
        "\n",
        "HYDE_PROMPT = \"\"\"Create a hypothetical answer to the following query: {query}\\n\\n\n",
        "          Format it to follow the structure of the template below:\\n\\n\n",
        "          {template}\\n\"\n",
        "          Ensure that the hypothetical answer does not reference new named entities that are not present in the original query.\"\"\""
      ],
      "metadata": {
        "id": "ljschCvTQ7yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CommunitySearch(Event):\n",
        "    query: str\n",
        "    hyde_query: str\n",
        "\n",
        "class LocalSearch(Event):\n",
        "    query: str\n",
        "    local_query: str\n",
        "\n",
        "class LocalSearchResults(Event):\n",
        "    results: dict\n",
        "\n",
        "class FinalAnswer(Event):\n",
        "    query: str"
      ],
      "metadata": {
        "id": "mPqi_dHG9d9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_RESPONSE_TYPE = \"multiple paragraphs\"\n",
        "LOCAL_TOP_K = 3\n",
        "MAX_LOCAL_SEARCH_DEPTH = 2\n",
        "\n",
        "\n",
        "class DriftSearch(Workflow):\n",
        "    @step\n",
        "    async def hyde_generation(self, ev: StartEvent) -> CommunitySearch:\n",
        "        random_community_report = driver.execute_query(\n",
        "            \"\"\"\n",
        "        MATCH (c:__Community__)\n",
        "        WHERE c.summary IS NOT NULL\n",
        "        RETURN coalesce(c.title, \"\") + \" \" + c.summary AS community_description\"\"\",\n",
        "            result_transformer_=lambda r: r.data(),\n",
        "        )\n",
        "        hyde = HYDE_PROMPT.format(\n",
        "            query=ev.query, template=random_community_report[0][\"community_description\"]\n",
        "        )\n",
        "        hyde_response = await client.responses.create(\n",
        "            model=\"gpt-5-mini\",\n",
        "            input=[{\"role\": \"user\", \"content\": hyde}],\n",
        "            reasoning={\"effort\": \"low\"},\n",
        "        )\n",
        "        return CommunitySearch(query=ev.query, hyde_query=hyde_response.output_text)\n",
        "\n",
        "    @step\n",
        "    async def community_search(self, ctx: Context, ev: CommunitySearch) -> LocalSearch:\n",
        "        embedding_response = await client.embeddings.create(\n",
        "            input=ev.hyde_query, model=TEXT_EMBEDDING_MODEL\n",
        "        )\n",
        "        embedding = embedding_response.data[0].embedding\n",
        "        community_reports = driver.execute_query(\n",
        "            \"\"\"\n",
        "        CALL db.index.vector.queryNodes('community', 5, $embedding) YIELD node, score\n",
        "        RETURN 'community-' + node.id AS source_id, node.summary AS community_summary\n",
        "        \"\"\",\n",
        "            result_transformer_=lambda r: r.data(),\n",
        "            embedding=embedding,\n",
        "        )\n",
        "        initial_prompt = DRIFT_PRIMER_PROMPT.format(\n",
        "            query=ev.query, community_reports=community_reports\n",
        "        )\n",
        "        initial_response = await client.responses.create(\n",
        "            model=\"gpt-5-mini\",\n",
        "            input=[{\"role\": \"user\", \"content\": initial_prompt}],\n",
        "            reasoning={\"effort\": \"low\"},\n",
        "        )\n",
        "        response_json = json_repair.loads(initial_response.output_text)\n",
        "        print(f\"Initial intermediate response: {response_json['intermediate_answer']}\")\n",
        "        # Set global states\n",
        "        async with ctx.store.edit_state() as ctx_state:\n",
        "            ctx_state[\"intermediate_answers\"] = [\n",
        "                {\n",
        "                    \"intermediate_answer\": response_json[\"intermediate_answer\"],\n",
        "                    \"score\": response_json[\"score\"],\n",
        "                }\n",
        "            ]\n",
        "            ctx_state[\"local_search_num\"] = len(response_json[\"follow_up_queries\"])\n",
        "        # Run follow-up queries in parallel\n",
        "        for local_query in response_json[\"follow_up_queries\"]:\n",
        "            ctx.send_event(LocalSearch(query=ev.query, local_query=local_query))\n",
        "        return None\n",
        "\n",
        "    @step(num_workers=5)\n",
        "    async def local_search(self, ev: LocalSearch) -> LocalSearchResults:\n",
        "        print(f\"Running local query: {ev.local_query}\")\n",
        "        response = await client.embeddings.create(\n",
        "            input=ev.local_query, model=TEXT_EMBEDDING_MODEL\n",
        "        )\n",
        "        embedding = response.data[0].embedding\n",
        "        local_reports = driver.execute_query(\n",
        "            \"\"\"\n",
        "    CALL db.index.vector.queryNodes('entity', 5, $embedding) YIELD node, score\n",
        "    WITH collect(node) AS nodes\n",
        "    WITH\n",
        "  collect {\n",
        "    UNWIND nodes as n\n",
        "    MATCH (n)<-[:MENTIONS]->(c:__Chunk__)\n",
        "    WITH c, count(distinct n) as freq\n",
        "    RETURN {chunkText: c.text, source_id: 'chunk-' + c.id}\n",
        "    ORDER BY freq DESC\n",
        "    LIMIT 3\n",
        "} AS text_mapping,\n",
        "collect {\n",
        "    UNWIND nodes as n\n",
        "    MATCH (n)-[:IN_COMMUNITY*]->(c:__Community__)\n",
        "    WHERE c.summary IS NOT NULL\n",
        "    WITH c, c.rating as rank\n",
        "    RETURN {summary: c.summary, source_id: 'community-' + c.id}\n",
        "    ORDER BY rank DESC\n",
        "    LIMIT 3\n",
        "} AS report_mapping,\n",
        "collect {\n",
        "    UNWIND nodes as n\n",
        "    MATCH (n)-[r:SUMMARIZED_RELATIONSHIP]-(m)\n",
        "    WHERE m IN nodes\n",
        "    RETURN {descriptionText: r.summary, source_id: 'relationship-' + n.name + '-' + m.name}\n",
        "    LIMIT 3\n",
        "} as insideRels,\n",
        "collect {\n",
        "    UNWIND nodes as n\n",
        "    RETURN {descriptionText: n.summary, source_id: 'node-' + n.name}\n",
        "} as entities\n",
        "RETURN {Chunks: text_mapping, Reports: report_mapping,\n",
        "       Relationships: insideRels,\n",
        "       Entities: entities} AS output\n",
        "    \"\"\",\n",
        "            result_transformer_=lambda r: r.data(),\n",
        "            embedding=embedding,\n",
        "        )\n",
        "        local_prompt = DRIFT_LOCAL_SYSTEM_PROMPT.format(\n",
        "            response_type=DEFAULT_RESPONSE_TYPE,\n",
        "            context_data=local_reports,\n",
        "            global_query=ev.query,\n",
        "        )\n",
        "        local_response = await client.responses.create(\n",
        "            model=\"gpt-5-mini\",\n",
        "            input=[{\"role\": \"user\", \"content\": local_prompt}],\n",
        "            reasoning={\"effort\": \"low\"},\n",
        "        )\n",
        "        response_json = json_repair.loads(local_response.output_text)\n",
        "        # Trim to topK\n",
        "        response_json[\"follow_up_queries\"] = response_json[\"follow_up_queries\"][\n",
        "            :LOCAL_TOP_K\n",
        "        ]\n",
        "        return LocalSearchResults(results=response_json, query=ev.query)\n",
        "\n",
        "    @step\n",
        "    async def local_search_results(\n",
        "        self, ctx: Context, ev: LocalSearchResults\n",
        "    ) -> LocalSearch | FinalAnswer:\n",
        "        local_search_num = await ctx.store.get(\"local_search_num\")\n",
        "        results = ctx.collect_events(ev, [LocalSearchResults] * local_search_num)\n",
        "        if results is None:\n",
        "            return None\n",
        "        intermediate_results = [\n",
        "            {\n",
        "                \"intermediate_answer\": event.results[\"response\"],\n",
        "                \"score\": event.results[\"score\"],\n",
        "            }\n",
        "            for event in results\n",
        "        ]\n",
        "        current_depth = await ctx.store.get(\"local_search_depth\", default=1)\n",
        "        # Parse out original query\n",
        "        query = [ev.query for ev in results][0]\n",
        "\n",
        "        if current_depth < MAX_LOCAL_SEARCH_DEPTH:\n",
        "            await ctx.store.set(\"local_search_depth\", current_depth + 1)\n",
        "            follow_up_queries = [\n",
        "                query\n",
        "                for event in results\n",
        "                for query in event.results[\"follow_up_queries\"]\n",
        "            ]\n",
        "            # Set global states\n",
        "            async with ctx.store.edit_state() as ctx_state:\n",
        "                ctx_state[\"intermediate_answers\"].extend(intermediate_results)\n",
        "                ctx_state[\"local_search_num\"] = len(follow_up_queries)\n",
        "\n",
        "            for local_query in follow_up_queries:\n",
        "                ctx.send_event(LocalSearch(query=query, local_query=local_query))\n",
        "            return None\n",
        "        else:\n",
        "            return FinalAnswer(query=query)\n",
        "\n",
        "    @step\n",
        "    async def final_answer_generation(self, ctx: Context, ev: FinalAnswer) -> StopEvent:\n",
        "        intermediate_answers = await ctx.store.get(\"intermediate_answers\")\n",
        "        answer_prompt = DRIFT_REDUCE_PROMPT.format(\n",
        "            response_type=DEFAULT_RESPONSE_TYPE,\n",
        "            context_data=intermediate_answers,\n",
        "            global_query=ev.query,\n",
        "        )\n",
        "        answer_response = await client.responses.create(\n",
        "            model=\"gpt-5-mini\",\n",
        "            input=[\n",
        "                {\"role\": \"developer\", \"content\": answer_prompt},\n",
        "                {\"role\": \"user\", \"content\": ev.query},\n",
        "            ],\n",
        "            reasoning={\"effort\": \"low\"},\n",
        "        )\n",
        "\n",
        "        return StopEvent(result=answer_response.output_text)"
      ],
      "metadata": {
        "id": "duIuA_rn-wZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_question = \"\"\"How do the various characters Alice encounters in Wonderland, such as the Cheshire Cat, the Caterpillar,\n",
        "and the Queen of Hearts, challenge her understanding of logic, rules, and authority, and what does this reveal about the nature of the adult world she is entering?\"\"\"\n",
        "\n",
        "w = DriftSearch(timeout=3600, verbose=False)\n",
        "result = await w.run(query=global_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcdV_2JYDcYi",
        "outputId": "e4b8c692-89e5-450a-cbfc-ad7a7e16565a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial intermediate response: # How this relates to the query\n",
            "\n",
            "Alice's encounters in Wonderland stage a series of vignettes that unsettle her (and the reader's) assumptions about logic, rules, and authority. The Cheshire Cat undermines consistent logic by appearing and disappearing at will and by speaking in riddling paradoxes; its grin and detachment suggest that identity and reason can be fragmentary. The Caterpillar challenges Alice's sense of stable self and taxonomy — his questions about who she is and his languid, cryptic counsel force Alice to confront the malleability of identity and the limits of straightforward instruction. The Queen of Hearts institutionalizes arbitrary authority: her frequent cries of \"Off with their heads!\" and the mock-legal procedures of the trial expose how power can be performative, capricious, and divorced from justice.\n",
            "\n",
            "Together these figures reveal an adult world where rules exist but lack coherent grounding. Logic becomes local and conversational rather than universal; rules are enacted as spectacles (croquet, trial) in which the instruments and participants are often absurd (flamingos, hedgehogs, playing-card jurors). Authority is theatrical: the Queen rules by intimidation and ritual rather than reasoned governance, while the King and functionaries mimic legal forms without integrity. The domestic figures (Duchess, cook) and servants (White Rabbit) show how hierarchy permeates daily life, yet their behaviors are exaggerated to emphasize the dissonance between adult institutions and childish sense of fairness.\n",
            "\n",
            "Alice responds by experimenting with logic, testing rules, and asserting her own judgement. Her growth consists less in adopting adult certainties than in learning to navigate an incoherent social order, signaling that maturity may demand pragmatic flexibility and critical distance rather than simple compliance.\n",
            "Running local query: How does the Cheshire Cat's philosophy compare to contemporary skepticism?\n",
            "Running local query: In what scenes does Carroll satirize Victorian legal and social institutions most directly?\n",
            "Running local query: How does Alice's changing size function as a metaphor for navigating authority?\n",
            "Running local query: What role do secondary characters like the White Rabbit and Duchess play in illustrating social hierarchies?\n",
            "Running local query: How do linguistic puzzles in the book undermine conventional logic and adult language?\n",
            "Running local query: How does Carroll use size changes (shrinking and growing) as a metaphor for psychological or social transitions between childhood and adulthood?\n",
            "Running local query: What specific dialogue moments show Alice successfully using logic to resist absurd authority, and what does that tell us about the limits of her success?\n",
            "Running local query: How do the Queen of Hearts’ procedures (trial, execution calls) contrast with the verbal pedantry of characters like the Mouse or Caterpillar in representing adult power?\n",
            "Running local query: How do specific episodes (e.g., the Mad Tea-Party, the trial of the Knave of Hearts) each illustrate different facets of adult absurdity and authority?\n",
            "Running local query: What role does language (nonsense verse, riddles, misused words) play in undermining logical reasoning in Wonderland?\n",
            "Running local query: How does Alice’s emotional and cognitive development across the episodes reflect her negotiation with adult norms?\n",
            "Running local query: Can you extract and summarise the passages about the Caterpillar and the Duchess from the primary text to compare their approaches to logic and authority with those of the Queen and King?\n",
            "Running local query: How does Carroll use language (wordplay, puns, paradoxes) in specific scenes to destabilise rational argument—can you provide close readings of two or three key exchanges?\n",
            "Running local query: What parallels can be drawn between the courtroom satire in Alice and real Victorian legal practices or public anxieties about institutions?\n",
            "Running local query: How does Alice herself change in response to these encounters—does she adopt any of the adults' tactics, or does she retain a childlike standard of logic?\n",
            "Running local query: What role does humor and nonsense play in Carroll’s critique of Victorian social institutions?\n",
            "Running local query: Can parallels be drawn between Wonderland’s institutions (court, croquet-game) and specific Victorian legal or social practices?\n",
            "Running local query: How does the Cheshire Cat’s habit of appearing and disappearing, and its conversational tone, specifically affect Alice’s trust in guidance and moral counsel?\n",
            "Running local query: In what ways do courtroom and legal parodies (trial scenes) in Alice mirror real 19th-century legal or social institutions?\n",
            "Running local query: How does Alice’s use of logic and questions evolve across the story—does she become more worldly or more resolute in her child logic?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhqsiHmNQRyd",
        "outputId": "421f9352-d671-4456-cd7d-4ef36532b7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary\n",
            "\n",
            "Alice’s encounters in Wonderland repeatedly unsettle her expectations about consistent logic, stable rules, and legitimate authority. Key figures—the Cheshire Cat, the Caterpillar, and the Queen of Hearts—each expose different ways that meaning, measurement, and power in the adult world can be arbitrary, performative, or detached from responsibility. These episodes push Alice toward a skeptical, adaptive stance rather than simple acceptance of adult rules [Data: Entities (node-ALICE); Reports (community-2-9)].\n",
            "\n",
            "How specific characters challenge Alice\n",
            "\n",
            "- Cheshire Cat — instability of identity and guidance: The Cat appears and disappears at will (sometimes leaving only a grin), offering ambiguous directional or philosophical remarks rather than clear guidance. Its vanishing/returning undermines Alice’s trust that an authoritative speaker or guide will be consistently present or accountable [Data: Entities (node-CHESHIRE CAT, node-CAT); Reports (community-2-4)].\n",
            "\n",
            "- Caterpillar — language, measurement, and contested selfhood: The Caterpillar interrogates Alice’s words and asks pointedly about her size, treating identity and correct speech as negotiable. This scene shows how authority can operate by policing definitions and measurements, making supposedly objective categories (like “size” or “correct words”) contingent and subject to condescension rather than reasoned explanation [Data: Chunks (chunk-df8bc33a7c9ecba34d8166eb4f107e65); node-ALICE-CATERPILLAR CONVERSATION].\n",
            "\n",
            "- Queen of Hearts — performative, arbitrary justice: The Queen epitomizes theatrical authority: she pronounces immediate punishments (“Off with their heads!”) and treats legal forms as spectacle. The court and croquet-ground scenes turn rules into ritual props (trumpets, parchments, playing-card officers), demonstrating power enforced by intimidation and ceremony rather than by fair procedure or rational principle [Data: Reports (community-2-9); Entities (node-QUEEN); node-KING].\n",
            "\n",
            "What this reveals about the adult world Alice is entering\n",
            "\n",
            "- Rules as contingent performance: Adults in Wonderland often proclaim rules or enact procedures that lack principled grounding; authority depends on ritual and assertion more than consistency. The court’s mock-legal procedures and the Queen’s preference for “sentence first—verdict afterward” show laws functioning as theatrical props rather than instruments of justice [Data: Reports (community-2-9); Entities (node-KING); chunk-d8d98640dadd4328f03d17aac05f55ee].\n",
            "\n",
            "- Language and categories as instruments of control: Encounters that twist meanings (the Caterpillar’s corrections, argumentative dialogues) illustrate how language can be used to confuse or dominate rather than to clarify, making adult communicative orders unreliable for settling disputes about facts or identity [Data: Chunks (chunk-77f392cce7770ae32ed71bef9c798585); node-ALICE-CATERPILLAR CONVERSATION].\n",
            "\n",
            "- Authority separated from responsibility: The Cheshire Cat’s detached presence and the court’s ceremonial officers suggest that people who hold or pronounce power may be ontologically or morally unaccountable—able to command without being present in a way that justifies their commands [Data: Relationships (relationship-CHESHIRE CAT-CAT'S HEAD); Reports (community-2-4)].\n",
            "\n",
            "Implications and brief commentary\n",
            "\n",
            "Taken together, these episodes satirize adult institutions by showing how social order can be upheld through spectacle, rhetorical force, and arbitrary enforcement rather than through coherent principles. Alice’s responses—questioning, pointing out contradictions, and refusing to accept nonsense at the trial—model a critical, adaptive approach to adulthood: maturity here is less about internalizing adult certainties and more about learning to navigate and resist incoherent authority [Data: Entities (node-ALICE); Reports (community-2-21)].\n",
            "\n",
            "(If you’d like, I can cite specific scene excerpts for each episode or map these patterns onto Victorian social/legal institutions for a deeper historical reading.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugPlXvsaCaTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}